
This paper will visit the development, training and evaluation of an AI prediction model. The model is to be used in a clinical setting regarding knee osteoarthritis treatment. A successful and effective treatment is the insertion of a Total Knee Arthroplasty (TKA). The number of TKA insertions are high in Denmark and it is expected to increase. TKA insertions are not always satisfactory and 5-10\% will need revision within 10 years. The objective of the model is indivdualized patient selection to identify patient's risk group. This enables optimization before, under and after surgery, tailoring  individual treatment plans and thereby increase the chance of the TKA surviving\cite{problem-description}   .           

The paper is done as a project in the course Computer Vision and Machine Learning at Aarhus University. The dataset is given together with an explanation of the features and description of the problem. The dataset is in advance split in training, validation and test sets. The training and validation observations are labeled. As the model is to determine wether a patient belongs in a risk group or not, it is a supervised binary classification problem.     

\subsection{Imbalanced Data}
Building a model with imbalanced data can lead to problems. As the model primarily sees the majority class it might not learn enough from the minority class. This can lead to a model predicting all new observations as the majority class. This will lead to a high accuracy and recall for the majority class but a recall of 0 on the minority class. This is basically an useless model, that wont predict any of the minority class observations. In the clinical setting this means that we wont place any patients in the risk group. There are several ways to mitigate the issues with imbalanced data. The majority class can be downsampled, the minority class can be oversampled or for certain models we can tune the importance of each class during training \cite*{svm-imbalanced-data}.

\subsection{Support Vector Machine}
The model that will be used is a Support Vector Machine(SVM). It produces nonlinear boundaries by creating a linear boundary in a tranformed version of the feature space. With this functionality SVMs can be used for classification, regression and outlier detection. We will use it for classification. In its simplest form the SVM creates an linear optimal seperating hyperplane between two seperated classes. This is done by maximizing the margin between each support vector. Where classes can not be seperated by a linear boundary other kernels(polynomial, Radial Basis Function) can be utilized. There are hyper-parameters to be tuned of the SVM. The most important is the regularization parameter that determines the amount of punishment for misclassifications. Utilizing a polynomial kernel the degree of the kernel function is also to be tuned \cite*{statistical-book}. Default SVM does not perform well on imbalanced data, as the margin will favor the majority class. However, SVM can be extended to account for the  importance of each class when maximizing the margin. The modified model is also reffered to as weighted SVM \cite*{svm-imbalanced-data}.
   
\subsection{Feature Selection}
The dataset contains a range of features regarding the patients. Having data with a high dimensionality can lead to problems such as long training time, a complex model and overfitting. Therefore, it will prefered if we can reduce the dimensionality while still having good performance of the model. This can be done by selecting the features that contribute the most to the target variable. This is typically supervised and will keep features intact. Principal Component Analysis can also be used to reduce the amount of features. This will however transform the features based on variance. In regards to a clinical settings it would be interesting to see which features are the most valuable in terms of determining wether an patient is in the risk group.
